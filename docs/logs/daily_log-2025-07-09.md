# Video Scripter — Working Dev Log (2025-07-09)
- This gets refreshed daily and the core info is saved to condensed logs
- Goal is to give Claude good active context for what we are working on

## 📌 Project Overview
Video Scripter is a Next.js 15 application for analyzing YouTube videos and creating content using AI. Features comprehensive video analysis pipeline with the "Skyscraper" framework, vector database integration, and multi-phase workflow for content creation.

## 🎯 Current Status
- **Database**: 208 user videos + 45,805+ competitor videos from 311+ channels 
- **Semantic Search**: 45,805 videos fully embedded (100% coverage) with Pinecone vector database
- **Performance**: Packaging analysis optimized with <200ms response times (41x improvement via materialized views)
- **Analytics Dashboard**: Optimized with materialized views for instant loading (<1s vs 17s+)
- **Competitor Analysis**: Full system with import/refresh capabilities for competitive intelligence
- **Discovery System**: Complete 7-method discovery system with search-based discovery and RSS baseline calculation
- **Channel Import Pipeline**: Automated discovery → review → approval → import workflow operational
- **RSS Monitoring**: 98.8% coverage with duplicate filtering
- **Rolling Baselines**: Automated pg_cron processing with 45,805 videos calculated in 23 minutes
- **Content Categorization**: 492 BERTopic clusters with 29,594 videos categorized
- **Unified Import System**: Single VideoImportService handling all 8 import sources with dual embeddings
- **Asynchronous Processing**: Background worker queue system ready for 50K+ videos/day

## 🧪 Today's Work (2025-07-09)

### [1] Film Booth Video Ideation & Scripting Process Integration
- **Background**: Film Booth Level 1 course provides comprehensive video ideation and scripting methodology for YouTube success
- **Achievement**: Successfully analyzed Film Booth document and created step-by-step process guide for application integration
- **Process Summary**: 5-phase methodology from research → ideation → planning → scripting → production
- **Key Components**: BENS framework (Big, Easy, New, Safe), pattern bank research, Skyscraper method, brick system scripting

**Technical Implementation:**
- **Document Analysis**: Extracted and processed Film Booth .docx file containing complete course methodology
- **Process Documentation**: Created comprehensive `/film_booth/Video_Ideation_Scripting_Process.md` summarizing key steps
- **Pattern Recognition**: Emphasized outlier analysis and pattern banks for data-driven content creation
- **Script Structure**: Documented brick system (Intro → Middle → End) with specific timing and content requirements

**Film Booth Process Overview:**
1. **Research & Pattern Recognition**: Build pattern banks from successful "outlier" videos, analyze competitor channels
2. **Ideation Process**: Generate titles/thumbnails from patterns, validate with community feedback
3. **Content Planning**: Use Skyscraper method to improve on existing successful content
4. **Script Writing**: Apply brick system with 15-second intros, structured middle sections, clear CTAs
5. **Production Preparation**: Batch filming approach for efficiency

**Integration Points with Video Scripter:**
- **Pattern Bank → Database**: Store outlier videos and patterns in existing competitor analysis system
- **BERTopic Clusters → Content Categories**: Align Film Booth viewer types with discovered topic clusters
- **Performance Data → BENS Validation**: Use existing performance metrics to validate BENS principles
- **Skyscraper Method → AI Analysis**: Enhance existing Skyscraper framework with Film Booth methodology

## 📋 Next Steps
- Integrate Film Booth pattern bank methodology with existing competitor analysis system
- Build UI components for outlier identification and pattern tracking
- Enhance script templates with Film Booth brick system structure
- Create workflow for validating titles/thumbnails against BENS framework
- Implement Skyscraper method enhancements for content planning phase

## 🎯 Technical Achievements
- Film Booth methodology documentation completed
- Process integration points identified with existing system
- Foundation laid for enhanced video creation workflow

### [2] BERTopic Cluster Integration Strategy for Search & Discovery
- **Background**: With 397 BERTopic clusters successfully assigned to database, explored UI integration strategies
- **Challenge**: How to surface cluster insights without exposing technical cluster IDs to users
- **Solution**: Dynamic cluster naming system that auto-generates meaningful names from content patterns
- **Key Decision**: Generate human-readable cluster names during analysis rather than showing raw numbers

**Integration Strategy:**
1. **Search Tab Enhancement**: Group semantic search results by topic cluster to show "content neighborhoods"
2. **Packaging Tab Enhancement**: Add topic filtering without showing cluster numbers directly
3. **New Tool Concept**: "Content Gap Finder" - identifies underserved, high-performing content areas

**Dynamic Cluster Naming Approach:**
- **Auto-generate names** from top video titles/patterns in each cluster (e.g., "DIY Workshop Tours", "Tool Comparison Battles")
- **Store in database** with cluster metadata for easy updates
- **Update names** when clusters evolve significantly (>30% content drift)
- **Examples**: "Quick Workshop Tips", "Restoration Time-lapses", "Deep Dive Tutorials"

**Technical Implementation:**
- Create `cluster_metadata` table to store names, keywords, performance stats
- Generate names by analyzing common patterns in titles, formats, and themes
- Track cluster evolution over time with name history
- Allow manual override for well-understood clusters

**Benefits:**
- Users see meaningful categories like "Shop Organization Guides" instead of "Cluster 47"
- Clusters remain flexible and data-driven while being user-friendly
- Can show insights like "Trending in 'Tool Reviews'" or "'Restoration Projects' is underserved"

**Next Steps:**
- Create cluster naming algorithm to process existing 397 clusters
- Design cluster metadata table schema
- Update search/packaging APIs to include cluster names
- Build Content Gap Finder tool leveraging named clusters

**Note**: Can use existing BERTopic analysis from 2025-07-08 - no need to rerun. Just need to add naming layer on top.

### [3] Video Processing Worker Reliability Improvements
- **Issue**: Worker process was showing stuck jobs after restarts/crashes
- **Root Cause**: Jobs marked as "processing" remained stuck when workers crashed unexpectedly
- **Solution**: Implemented automatic stuck job recovery on worker startup and during runtime

**Technical Implementation:**
- **Startup Cleanup**: Worker now checks for jobs stuck in "processing" state > 30 minutes on startup
- **Runtime Monitoring**: Periodic checks (10% probability per poll) to detect and reset stuck jobs
- **Graceful Recovery**: Stuck jobs automatically reset to "pending" with error message for tracking
- **Helper Script**: Created `scripts/reset-stuck-jobs.js` for manual intervention if needed

**Code Changes:**
- Added `cleanupStuckJobs()` method to worker.ts
- Integrated cleanup into startup sequence and polling loop
- Jobs stuck > 30 minutes automatically recover
- Prevents job queue blockage from worker crashes

**Benefits:**
- Zero manual intervention required for stuck jobs
- Worker restarts automatically recover abandoned work
- Improved reliability for 50K+ videos/day processing target
- Better visibility into job processing issues

### [4] Unified Import System Enhancement - Direct URL Import for Competitors
- **Background**: Users wanted to paste YouTube channel URLs directly instead of searching via API (saves 100 units/search)
- **Challenge**: Unified import had hardcoded 50-video limit preventing full channel imports
- **Solution**: Enhanced unified import with pagination and competitor-specific filtering options

**Technical Implementation:**
- **URL Scraping Endpoint**: Created `/api/youtube/scrape-channel` to extract channel IDs from URLs without API calls
- **Unified Import Pagination**: Added full pagination support to `fetchVideosFromChannels()` to get ALL videos
- **Competitor Options**: Added timePeriod, excludeShorts, and userId options to VideoImportRequest interface
- **Simplified Architecture**: Removed fallback mechanism - competitor import now cleanly delegates to unified import

**Key Changes:**
1. **Web Scraping**: Extracts channel IDs from multiple URL formats (/@handle, /channel/, /c/, /user/)
2. **Full Channel Import**: Pagination ensures ALL videos imported, not just first 50
3. **Time Period Filtering**: Supports date-based filtering (though UI uses 'all' for competitors)
4. **Shorts Exclusion**: Filters YouTube Shorts during import based on duration
5. **Clean Delegation**: Competitor import endpoint now just calls unified import with proper options

**Architecture Improvements:**
- No more confusing fallback mechanisms
- Performance ratios left to database calculation via `rolling_baseline_views`
- Single source of truth for all video imports
- Maintains all benefits: embeddings, exports, proper metadata

**User Experience:**
- Paste channel URL → Import all videos with one click
- No YouTube API quota waste on searches
- Same import quality as manual search → select → import flow
- Works with all YouTube URL formats

### [5] Terminal Output Clarity Improvements
- **Issue**: Confusing terminal logs when importing channels - unclear if async/sync, how many videos being processed
- **Solution**: Enhanced logging throughout the import pipeline for better visibility

**Improvements Made:**
1. **Clear Mode Indication**: Shows whether import is synchronous or async with worker queue
2. **Progress Tracking**: Shows pagination progress ("Fetching more videos (150 so far)...")
3. **Distinct Worker Logs**: Worker logs prefixed with "WORKER:" for clarity
4. **Import Summary**: Clear completion message with totals

**New Terminal Output Flow:**
```
🎯 Starting competitor import for channel UC79q...
⚡ Processing competitor import immediately (synchronous mode)
📊 Input: 1 channels, 0 videos
🎬 Will exclude YouTube Shorts

🔍 Fetching ALL videos from channel: UC79q...
📄 Fetching videos page 1...
📄 Fetching more videos (50 so far)...
📄 Fetching more videos (100 so far)...
✅ Retrieved 312 videos (after filtering) from channel

📊 Total: 312 videos from 1 channel(s) ready for processing
💾 Storing 312 videos in database...
✅ Database storage complete

✅ IMPORT COMPLETE: 312 videos processed successfully
📊 Generated 312 title embeddings, 312 thumbnail embeddings
```

**Benefits:**
- No confusion about 50-video limits
- Clear indication when using worker vs synchronous processing
- Progress visibility during long imports
- Confirmation of successful completion with totals

### [6] Non-Blocking Competitor Import UI
- **Issue**: UI would hang/freeze while processing competitor imports, preventing users from starting new imports
- **Solution**: Switched competitor imports to use async worker queue instead of synchronous processing

**Technical Implementation:**
1. **Async Processing**: Changed `useQueue: false` to `useQueue: true` in competitor import endpoint
2. **UI Response Handling**: Updated UI to detect queued job responses vs sync responses
3. **Immediate Reset**: Form resets after 1 second when job is queued, allowing new imports
4. **Job Status**: Returns job ID and status URL for tracking progress

**Code Changes:**
- `/app/api/youtube/import-competitor/route.ts`: Now uses async queue by default
- `/app/dashboard/youtube/competitors/page.tsx`: Handles async responses differently
- Competitor import endpoint passes through unified import's job response

**User Experience Improvements:**
- Import starts → "Processing in background" toast → Form resets immediately
- Users can paste another URL or search for another channel right away
- No more UI freezing during large channel imports
- Multiple imports can run concurrently in background workers

**Example Flow:**
```
User pastes URL → Click Import → "Import Started" toast → Form clears → Ready for next import
(Meanwhile, worker processes 300+ videos in background)
```

### [7] Direct URL Import Duplicate Detection
- **Issue**: When pasting a YouTube URL, system would attempt import even if channel already existed
- **Solution**: Enhanced scrape-channel endpoint to check import status immediately after extracting channel ID

**Technical Implementation:**
- Modified `/api/youtube/scrape-channel/route.ts` to include import status check
- Added `checkChannelImportStatus()` function to check both competitor and discovery tables
- Fixed Supabase query to use `.maybeSingle()` instead of `.single()` to handle non-existent rows
- Returns `isAlreadyImported` and `importSource` in scrape response
- Added comprehensive logging for debugging duplicate detection flow

**Bug Fix:**
- Initial implementation used `.single()` which threw errors when no row found
- Browser caching prevented updated frontend code from running
- Solution: Changed to `.maybeSingle()` and required hard browser refresh for frontend changes

**User Experience:**
- Paste URL → Extract channel → Check if imported → Show error toast if duplicate
- No wasted API calls or processing for already-imported channels
- Clear messaging: "This channel is already in your discovery system" or "already imported as competitor"
- Prevents duplicate imports at the earliest possible stage
- Works identically to search-based import duplicate detection

### [8] Discovery Import System Async Queue Fix
- **Issue**: Discovery import failing when using unified import system with async queue - TypeError accessing undefined properties
- **Root Cause**: Import endpoint expected synchronous response properties (`embeddingsGenerated`, `videosProcessed`) on async job responses
- **Solution**: Added response type detection to handle both async job responses and sync processing responses differently

**Technical Implementation:**
- **Response Type Detection**: Check for `jobId` and `status: 'queued'` to identify async job responses
- **Conditional Handling**: Async responses return job info, sync responses return full processing results
- **Fixed Properties**: Job responses don't have `embeddingsGenerated.titles` or `videosProcessed`
- **UI Enhancement**: Show "Queued" instead of "Imported" for async jobs, display job ID

**Code Changes:**
1. **Primary Fix**: Added `isJobResponse` check in `/api/youtube/discovery/import-approved/route.ts`
2. **Channel Processing**: Fixed similar issue in `processChannel()` function for individual channel imports
3. **Frontend Update**: Enhanced UI to show job ID and differentiate queued vs completed imports

**Response Structure Differences:**
```typescript
// Async Job Response
{
  success: true,
  jobId: "uuid-here",
  status: "queued",
  message: "Job created..."
}

// Sync Processing Response
{
  success: true,
  videosProcessed: 312,
  embeddingsGenerated: { titles: 312, thumbnails: 312 },
  errors: []
}
```

**Benefits:**
- Discovery import now properly uses unified import system with worker queue
- No more TypeErrors when accessing undefined properties
- Clear feedback to users about background processing
- Maintains all unified system benefits: dual embeddings, exports, async processing

## 📊 System Performance
- All existing performance metrics maintained from 2025-07-08
- Film Booth document processing completed without system impact
- Ready for UI/workflow integration in subsequent phases
- BERTopic clusters ready for dynamic naming implementation
- Worker queue system enhanced with automatic recovery mechanisms
- Unified import system now handles full channel imports without limitations
- Terminal logging improved for clearer import progress tracking
- Competitor imports now non-blocking with async worker processing
- Discovery imports fixed to work with async queue system

### [9] Vectorization Workers with UI Control System
- **Background**: User needed dedicated workers for title and thumbnail vectorization with UI start/stop controls
- **Challenge**: Workers were automatically processing when started, needed UI-based control mechanism
- **Solution**: Implemented database-controlled worker system with dashboard integration

**Technical Implementation:**
1. **Separate Worker Processes**: Created `title-vectorization-worker.ts` and `thumbnail-vectorization-worker.ts`
   - Title worker: Processes 50 videos/batch every 60 seconds using OpenAI embeddings
   - Thumbnail worker: Processes 20 thumbnails/batch every 90 seconds using Replicate CLIP
   - Both workers check database control state before processing

2. **Database Control System**: 
   - Created `worker_control` table to store enable/disable state
   - Workers poll this table to determine if they should process
   - Tracks last enabled/disabled timestamps

3. **API Endpoints**:
   - `GET/POST /api/workers/vectorization/control` - Control worker state
   - `GET /api/workers/vectorization/progress` - Check vectorization progress

4. **UI Integration**:
   - Added Enable/Disable buttons to Worker Dashboard
   - Shows real-time progress bars and statistics
   - Reduced dashboard refresh interval from 5s to 30s to minimize API spam

**Worker Commands:**
```bash
# Run all workers (import + vectorization)
npm run workers

# Run only vectorization workers  
npm run workers:vectorization

# Individual workers
npm run worker:title
npm run worker:thumbnail
```

**Architecture Benefits:**
- Workers run continuously but only process when enabled via UI
- No resource contention with main import worker
- Independent control of each vectorization type
- Progress tracking without client-side processing
- Clean separation of concerns

**User Experience:**
- Start workers with npm command → Workers wait idle
- Enable via dashboard → Workers begin processing
- Disable via dashboard → Workers pause immediately
- Real-time progress visible on dashboard

### [10] Vectorization Worker Performance Enhancement
- **Issue**: Title vectorization worker only processing 50 videos per minute due to 60-second polling interval
- **Root Cause**: Worker was waiting 60 seconds between batches instead of continuous processing
- **Solution**: Implemented continuous processing with adaptive rate limiting

**Technical Implementation:**
- **Continuous Processing Loop**: Workers now process batches continuously while enabled
- **Adaptive Rate Limiting**: 
  - Success: 500ms delay (titles), 3s delay (thumbnails)
  - Some errors: 2s delay (titles), 10s delay (thumbnails)  
  - High error rate (>50%): 10s delay (titles), 30s delay (thumbnails)
  - Exponential backoff on consecutive errors
- **Progress Tracking**: Shows processing rate (videos/min) every 500 videos or 100 thumbnails
- **Error Resilience**: Stops after 3 consecutive errors to prevent API hammering

**Performance Improvements:**
- Before: ~50 videos/minute (limited by 60s polling interval)
- After: ~1000-3000 videos/minute for titles (limited by API rate limits)
- After: ~200-400 thumbnails/minute (Replicate API has stricter limits)

**Code Changes:**
- Modified `processBatch()` to run in continuous loop
- Added adaptive delays based on error rates
- Added processing rate calculation and display
- Maintained ability to pause/resume via UI controls

### [11] Rate Limit Compliance for Vectorization Workers
- **Issue**: Workers needed proper rate limiting to avoid hitting OpenAI and Replicate API limits
- **Solution**: Implemented precise rate limiting based on documented API limits

**OpenAI Rate Limits (text-embedding-3-small):**
- Tier 1: 500 RPM, 200K TPM
- Tier 2: 3,500 RPM, 5M TPM  
- Tier 3: 5,000 RPM, 10M TPM
- Implementation assumes Tier 2 with conservative limits (3,000 RPM, 4M TPM)

**Replicate/Pinecone Limits:**
- Replicate: ~5-10 requests per second (conservative: 5 RPS)
- Pinecone: 
  - 4MB max request size per upsert
  - 10k max vectors per query
  - No documented rate limits (but code uses 100-200ms delays between batches)
  - Title embeddings: 50 vectors/batch (~200KB)
  - Thumbnail embeddings: 100 vectors/batch (~300KB) with automatic chunking

**Technical Implementation:**
1. **Title Worker Rate Limiting:**
   - Tracks requests and tokens per minute
   - Calculates remaining capacity before each batch
   - Waits until next minute if limits reached
   - Optimal delay calculation: `1000 / (RPM_LIMIT/60) * batch_size`

2. **Thumbnail Worker Rate Limiting:**
   - Tracks requests per second (Replicate has stricter limits)
   - Maximum 5 requests per second
   - Automatic wait when hitting per-second limits
   - Larger delays between batches due to processing time

**Benefits:**
- No more 429 rate limit errors
- Predictable processing throughput
- Maximizes API usage within safe limits
- Automatic recovery from temporary issues

### [12] UI Responsiveness Improvements for Worker Control
- **Issue**: Workers only checked enabled/disabled state every 60-90 seconds, causing slow response to UI commands
- **Root Cause**: Main worker loop had long polling intervals (60s for titles, 90s for thumbnails)
- **Solution**: Reduced polling frequency to 5 seconds for much faster UI responsiveness

**Technical Implementation:**
- **Faster State Checking**: Workers now check `isWorkerEnabled()` every 5 seconds instead of 60-90 seconds
- **Improved Responsiveness**: Start/stop commands from UI take effect within 5 seconds maximum
- **Maintained Performance**: Continuous processing still happens when enabled, just with more frequent control checks
- **Error Recovery**: Reduced error recovery time from 60-120 seconds to 10 seconds

**Code Changes:**
- Title worker: Changed from 60s polling to 5s polling
- Thumbnail worker: Changed from 90s polling to 5s polling
- Error handling: Reduced error backoff times for faster recovery
- Maintained all rate limiting and continuous processing features

**User Experience Improvements:**
- **Before**: Click "Enable" → Wait up to 60-90 seconds → Worker starts processing
- **After**: Click "Enable" → Wait ~5 seconds → Worker starts processing
- **Before**: Click "Disable" → Wait up to 60-90 seconds → Worker stops
- **After**: Click "Disable" → Wait ~5 seconds → Worker stops
- Much more responsive feel for start/stop controls
- Better feedback loop for users testing worker functionality

**Architecture Notes:**
- Workers still run continuously and check database flags (polling architecture)
- Alternative approaches considered: WebSocket triggers, process management API, queue-based signals
- Current polling approach chosen for simplicity and alignment with existing worker architecture
- 5-second polling provides good balance between responsiveness and database load

### [13] Worker Architecture Optimization - Eliminating API Contention
- **Issue**: Running multiple workers simultaneously caused severe performance degradation due to API rate limit contention
- **Root Cause**: Import worker and vectorization workers were hitting the same OpenAI/Replicate APIs simultaneously
- **Solution**: Implemented intelligent worker coordination to eliminate API competition

**Technical Implementation:**
1. **Smart Embedding Skip Logic**: Import worker now checks if vectorization workers are enabled before processing
2. **Automatic API Coordination**: When vectorization workers are active, import worker skips embeddings entirely
3. **Clean Worker Separation**: Each worker type now has dedicated API quota usage
4. **Database-Driven Coordination**: Uses existing `worker_control` table to coordinate between processes

**Code Changes:**
- **Import Worker Enhancement**: Added `checkVectorizationWorkers()` method to detect active vectorization workers
- **Conditional Processing**: Import worker automatically sets `skipTitleEmbeddings` and `skipThumbnailEmbeddings` when dedicated workers are enabled
- **Clear Logging**: Shows which workers are active and when embeddings are being skipped
- **Legacy Job Support**: Handles both new unified import jobs and legacy individual video jobs

**Worker Coordination Logic:**
```typescript
// Import worker checks vectorization worker status
const vectorizationStatus = await this.checkVectorizationWorkers();
if (vectorizationStatus.titleEnabled || vectorizationStatus.thumbnailEnabled) {
  console.log('⚡ Skipping embeddings during import to avoid API contention');
  importRequest.options.skipTitleEmbeddings = vectorizationStatus.titleEnabled;
  importRequest.options.skipThumbnailEmbeddings = vectorizationStatus.thumbnailEnabled;
}
```

**Performance Benefits:**
- **Before**: Multiple workers competing for same APIs → Severe slowdowns, rate limit errors
- **After**: Clean separation → Each worker type gets dedicated API quota
- **No Resource Waste**: No duplicate embedding generation
- **Faster Imports**: Import worker focuses purely on metadata/storage when vectorization workers handle embeddings
- **Better Throughput**: Each API gets optimal utilization without contention

**User Experience:**
- Run `npm run workers` → All workers start
- Enable vectorization workers via UI → Import worker automatically coordinates
- No configuration needed → System automatically optimizes for performance
- Clear terminal output showing coordination decisions