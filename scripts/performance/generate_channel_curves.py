#!/usr/bin/env python3
"""
Generate individual channel performance curves for channels with sufficient data
This creates channel-specific envelopes for more accurate performance analysis
"""

import os
import numpy as np
from supabase import create_client, Client
from dotenv import load_dotenv
from datetime import datetime
from scipy.ndimage import gaussian_filter1d

# Load environment variables
load_dotenv()

# Initialize Supabase client
url = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
supabase: Client = create_client(url, key)

print("üìä Generating Individual Channel Performance Curves")
print("=" * 60)

# Get channels ready for individual curves
print("üîç Finding channels with sufficient data...")
channels_query = """
SELECT 
    v.channel_id,
    v.channel_name,
    COUNT(*) as video_count,
    COUNT(vs.id) as snapshot_count,
    MIN(vs.days_since_published) as min_age,
    MAX(vs.days_since_published) as max_age
FROM videos v
JOIN view_snapshots vs ON v.id = vs.video_id
WHERE v.channel_id IS NOT NULL 
    AND v.duration IS NOT NULL
    AND v.duration != ''
    AND (v.duration LIKE '%H%' OR 
         (v.duration LIKE '%M%' AND v.duration LIKE 'PT%') OR
         (v.duration LIKE 'PT%S' AND v.duration NOT LIKE '%M%' AND 
          LENGTH(REPLACE(REPLACE(v.duration, 'PT', ''), 'S', '')) > 2))
GROUP BY v.channel_id, v.channel_name
HAVING COUNT(*) >= 30 
    AND COUNT(vs.id) >= 100
    AND MAX(vs.days_since_published) >= 90
ORDER BY COUNT(*) DESC
LIMIT 20;
"""

# Get channels directly using the select method
channels_result = supabase.table('videos')\
    .select('channel_id, channel_name')\
    .neq('channel_id', None)\
    .execute()

# Get ready channels with sufficient data (we already know these from our query)
ready_channels = [
    {'channel_id': 'UCBJycsmduvYEL83R_U4JriQ', 'channel_name': 'Marques Brownlee', 'video_count': 5408},
    {'channel_id': 'UC7ZddA__ewP3AtDefjl_tWg', 'channel_name': 'I Will Teach You To Be Rich', 'video_count': 2039},
    {'channel_id': 'UCGh9zg0zvyF3GqHeR4WR3Xg', 'channel_name': 'Cruise With Ben and David', 'video_count': 2024},
    {'channel_id': 'UCI8gcSTo1FowsRJdilsjsZw', 'channel_name': 'Grace For Purpose', 'video_count': 1960},
    {'channel_id': 'UC70SrI3VkT1MXALRtf0pcHg', 'channel_name': 'Thomas DeLauer', 'video_count': 1947}
][:5]  # Start with top 5 channels

print(f"Found {len(ready_channels)} channels ready for individual curves")

# Create channel_performance_envelopes table using MCP
print("\nüèóÔ∏è Setting up channel curves table...")
print("‚úÖ Using existing table or MCP tools for table creation")

# Process each channel
all_updates = []
processed_channels = 0

for channel in ready_channels:
    channel_id = channel['channel_id']
    channel_name = channel['channel_name']
    
    print(f"\nüìà Processing: {channel_name} ({channel['video_count']} videos)")
    
    # Get view snapshots for this channel with strategic day sampling
    key_days = []
    key_days.extend(range(0, 91, 1))    # Days 0-90: daily
    key_days.extend(range(91, 366, 3))  # Days 91-365: every 3 days
    key_days.extend(range(366, 1826, 7)) # Year 2-5: weekly
    key_days.extend(range(1826, 3651, 30)) # Year 6-10: monthly
    
    channel_stats = {}
    total_processed = 0
    
    for day in key_days:
        # Get view counts for this channel and day
        query_sql = f"""
        SELECT vs.view_count
        FROM view_snapshots vs
        JOIN videos v ON vs.video_id = v.id
        WHERE v.channel_id = '{channel_id}'
            AND vs.days_since_published = {day}
            AND v.duration IS NOT NULL
            AND v.duration != ''
            AND (v.duration LIKE '%H%' OR 
                 (v.duration LIKE '%M%' AND v.duration LIKE 'PT%') OR
                 (v.duration LIKE 'PT%S' AND v.duration NOT LIKE '%M%' AND 
                  LENGTH(REPLACE(REPLACE(v.duration, 'PT', ''), 'S', '')) > 2))
            AND vs.view_count IS NOT NULL
        """
        
        try:
            day_result = supabase.rpc('execute_sql', {'query': query_sql}).execute()
            if day_result.data:
                views = [row['view_count'] for row in day_result.data]
                total_processed += len(views)
                
                if len(views) >= 5:  # Minimum for reliable percentiles
                    views_array = np.array(views)
                    channel_stats[day] = {
                        'p10': int(np.percentile(views_array, 10)),
                        'p25': int(np.percentile(views_array, 25)),
                        'p50': int(np.percentile(views_array, 50)),
                        'p75': int(np.percentile(views_array, 75)),
                        'p90': int(np.percentile(views_array, 90)),
                        'p95': int(np.percentile(views_array, 95)),
                        'count': len(views)
                    }
        except Exception as e:
            print(f"   Error processing day {day}: {e}")
            continue
    
    if len(channel_stats) < 10:
        print(f"   ‚ùå Insufficient data points ({len(channel_stats)}), skipping")
        continue
    
    print(f"   ‚úÖ Processed {total_processed} snapshots across {len(channel_stats)} days")
    
    # Apply light smoothing
    days = sorted(channel_stats.keys())
    metrics = ['p10', 'p25', 'p50', 'p75', 'p90', 'p95']
    
    for metric in metrics:
        values = [channel_stats[d][metric] for d in days]
        # Light smoothing to preserve channel-specific patterns
        smooth_values = gaussian_filter1d(values, sigma=1.5)
        
        for i, day in enumerate(days):
            channel_stats[day][metric] = int(smooth_values[i])
    
    # Calculate confidence score based on data sufficiency
    total_samples = sum(stats['count'] for stats in channel_stats.values())
    confidence = min(1.0, total_samples / 500.0)  # Full confidence at 500+ samples
    
    # Interpolate missing days for full 10-year coverage
    all_days_data = {}
    for day in range(0, 3651):
        if day in channel_stats:
            all_days_data[day] = channel_stats[day]
            all_days_data[day]['confidence'] = confidence
        else:
            # Linear interpolation between nearest points
            before = max([d for d in days if d < day], default=0)
            after = min([d for d in days if d > day], default=days[-1])
            
            if before in channel_stats and after in channel_stats:
                weight = (day - before) / (after - before) if after != before else 0
                all_days_data[day] = {}
                
                for metric in metrics:
                    val = channel_stats[before][metric] + weight * (channel_stats[after][metric] - channel_stats[before][metric])
                    all_days_data[day][metric] = int(val)
                
                all_days_data[day]['count'] = 0  # Interpolated
                all_days_data[day]['confidence'] = confidence * 0.7  # Reduced confidence for interpolated
    
    # Prepare database updates
    current_time = datetime.now().isoformat()
    for day in range(0, 3651):
        if day in all_days_data:
            all_updates.append({
                'channel_id': channel_id,
                'day_since_published': day,
                'p10_views': all_days_data[day]['p10'],
                'p25_views': all_days_data[day]['p25'],
                'p50_views': all_days_data[day]['p50'],
                'p75_views': all_days_data[day]['p75'],
                'p90_views': all_days_data[day]['p90'],
                'p95_views': all_days_data[day]['p95'],
                'sample_count': all_days_data[day]['count'],
                'confidence_score': all_days_data[day]['confidence'],
                'updated_at': current_time
            })
    
    processed_channels += 1
    print(f"   üìä Channel curve ready: Day 1={all_days_data.get(1, {}).get('p50', 'N/A')}, Day 365={all_days_data.get(365, {}).get('p50', 'N/A')} views")

# Batch update database
print(f"\nüíæ Updating database with {len(all_updates)} curve points...")
batch_size = 500
batches_completed = 0

for i in range(0, len(all_updates), batch_size):
    batch = all_updates[i:i + batch_size]
    try:
        supabase.table('channel_performance_envelopes').upsert(batch).execute()
        batches_completed += 1
        print(f"   Batch {batches_completed}/{(len(all_updates)-1)//batch_size + 1} complete")
    except Exception as e:
        print(f"   ‚ùå Batch {batches_completed + 1} failed: {e}")

print(f"\n‚úÖ Individual Channel Curves Generation Complete!")
print(f"   Channels processed: {processed_channels}")
print(f"   Total curve points: {len(all_updates)}")
print(f"   Coverage: 10 years (3,651 days) per channel")
print(f"   Average confidence: {np.mean([u['confidence_score'] for u in all_updates]):.2f}")

# Show sample results
print(f"\nüìà Sample Results:")
sample_channels = ready_channels[:3]
for channel in sample_channels:
    channel_id = channel['channel_id']
    channel_name = channel['channel_name']
    
    # Get a few key points for this channel
    sample_query = f"""
    SELECT day_since_published, p50_views, confidence_score
    FROM channel_performance_envelopes
    WHERE channel_id = '{channel_id}'
        AND day_since_published IN (1, 7, 30, 365)
    ORDER BY day_since_published
    """
    
    try:
        sample_result = supabase.rpc('execute_sql', {'query': sample_query}).execute()
        if sample_result.data:
            print(f"\n   {channel_name}:")
            for row in sample_result.data:
                day = row['day_since_published']
                views = row['p50_views']
                conf = row['confidence_score']
                print(f"     Day {day}: {views:,} views (confidence: {conf:.2f})")
    except Exception as e:
        print(f"   Error getting sample for {channel_name}: {e}")