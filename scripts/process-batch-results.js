#!/usr/bin/env node

import { createClient } from '@supabase/supabase-js';
import OpenAI from 'openai';
import dotenv from 'dotenv';
import fs from 'fs/promises';
import path from 'path';

dotenv.config();

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
);

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

async function downloadBatchResults() {
  const batchInfoPath = path.join(process.cwd(), 'batch-jobs', 'batch-info.json');
  const batchInfo = JSON.parse(await fs.readFile(batchInfoPath, 'utf-8'));
  
  console.log('üì• Downloading batch results...\n');
  
  const allResults = [];
  
  for (const batch of batchInfo.batches) {
    try {
      const status = await openai.batches.retrieve(batch.batchId);
      
      if (status.status !== 'completed') {
        console.log(`‚è≥ Batch ${batch.batchId} is still ${status.status}, skipping...`);
        continue;
      }
      
      if (!status.output_file_id) {
        console.log(`‚ùå Batch ${batch.batchId} has no output file, skipping...`);
        continue;
      }
      
      console.log(`Downloading results for batch ${batch.batchId}...`);
      
      // Download the output file
      const fileResponse = await openai.files.content(status.output_file_id);
      const fileContent = await fileResponse.text();
      
      // Parse JSONL results
      const lines = fileContent.split('\n').filter(line => line.trim());
      const results = lines.map(line => JSON.parse(line));
      
      console.log(`  Found ${results.length} results`);
      
      // Save results locally
      const outputPath = path.join(process.cwd(), 'batch-jobs', `results-${batch.batchId}.jsonl`);
      await fs.writeFile(outputPath, fileContent);
      
      allResults.push(...results);
      
    } catch (error) {
      console.error(`Error processing batch ${batch.batchId}:`, error);
    }
  }
  
  console.log(`\n‚úÖ Downloaded ${allResults.length} total results`);
  return allResults;
}

async function updateDatabase(results) {
  console.log('\nüìù Updating database with summaries...');
  
  let successCount = 0;
  let errorCount = 0;
  const errors = [];
  
  // Process in batches of 100 for database updates
  const BATCH_SIZE = 100;
  
  for (let i = 0; i < results.length; i += BATCH_SIZE) {
    const batch = results.slice(i, i + BATCH_SIZE);
    
    const updates = batch.map(result => {
      // Check if the request was successful
      if (result.error) {
        errorCount++;
        errors.push({
          videoId: result.custom_id,
          error: result.error
        });
        return null;
      }
      
      const summary = result.response?.body?.choices?.[0]?.message?.content?.trim();
      
      if (!summary) {
        errorCount++;
        errors.push({
          videoId: result.custom_id,
          error: 'No summary in response'
        });
        return null;
      }
      
      return {
        id: result.custom_id,
        llm_summary: summary,
        llm_summary_generated_at: new Date().toISOString(),
        llm_summary_model: 'gpt-4o-mini'
      };
    }).filter(u => u !== null);
    
    if (updates.length > 0) {
      // Update videos in batch
      const { error } = await supabase
        .from('videos')
        .upsert(updates, { 
          onConflict: 'id',
          ignoreDuplicates: false 
        });
      
      if (error) {
        console.error('Database update error:', error);
        errorCount += updates.length;
      } else {
        successCount += updates.length;
      }
    }
    
    console.log(`Progress: ${Math.min(i + BATCH_SIZE, results.length)}/${results.length}`);
  }
  
  // Save errors for review
  if (errors.length > 0) {
    await fs.writeFile(
      path.join(process.cwd(), 'batch-jobs', 'errors.json'),
      JSON.stringify(errors, null, 2)
    );
  }
  
  return { successCount, errorCount };
}

async function generateEmbeddings() {
  console.log('\nüîÑ Generating embeddings for new summaries...');
  
  // Get videos with summaries but no embeddings
  const { data: videos, error } = await supabase
    .from('videos')
    .select('id, llm_summary')
    .not('llm_summary', 'is', null)
    .is('llm_summary_embedding_synced', false)
    .limit(1000); // Process in chunks
  
  if (error || !videos || videos.length === 0) {
    console.log('No summaries need embeddings');
    return 0;
  }
  
  console.log(`Found ${videos.length} summaries needing embeddings`);
  
  // This would normally call the embedding generation service
  // For now, just mark them as needing processing
  console.log('‚ö†Ô∏è  Embeddings will be generated by the standard sync process');
  
  return videos.length;
}

async function main() {
  console.log('ü§ñ LLM Summary Batch Results Processor\n');
  
  try {
    // Download all results
    const results = await downloadBatchResults();
    
    if (results.length === 0) {
      console.log('No results to process');
      return;
    }
    
    // Update database
    const { successCount, errorCount } = await updateDatabase(results);
    
    console.log('\nüìä Summary:');
    console.log(`‚úÖ Successfully updated: ${successCount.toLocaleString()} videos`);
    if (errorCount > 0) {
      console.log(`‚ùå Errors: ${errorCount} (see batch-jobs/errors.json)`);
    }
    
    // Note about embeddings
    const embeddingCount = await generateEmbeddings();
    if (embeddingCount > 0) {
      console.log(`\n‚ö†Ô∏è  ${embeddingCount} summaries need embeddings`);
      console.log('Run sync-summary-embeddings.js to generate embeddings');
    }
    
  } catch (error) {
    console.error('Error:', error);
  }
}

main().catch(console.error);