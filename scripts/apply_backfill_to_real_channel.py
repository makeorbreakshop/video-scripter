#!/usr/bin/env python3

"""
Apply Historical Backfill to Real Channel
Tests ML-enhanced confidence bands on "I Like To Make Stuff" or similar sparse-data channels
"""

import json
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class RealChannelBackfillTester:
    def __init__(self, model_timestamp="20250806_100239"):
        self.model_timestamp = model_timestamp
        self.model = None
        self.preprocessors = None
        self.metadata = None
        
    def load_models(self):
        """Load trained historical backfill models"""
        print("ðŸ¤– Loading historical backfill models...")
        
        # Load model
        model_path = f'models/historical_backfill_model_{self.model_timestamp}.pkl'
        with open(model_path, 'rb') as f:
            self.model = pickle.load(f)
        
        # Load preprocessors
        preprocessing_path = f'models/historical_backfill_preprocessing_{self.model_timestamp}.pkl'
        with open(preprocessing_path, 'rb') as f:
            self.preprocessors = pickle.load(f)
        
        # Load metadata
        metadata_path = f'models/historical_backfill_metadata_{self.model_timestamp}.json'
        with open(metadata_path, 'r') as f:
            self.metadata = json.load(f)
        
        print(f"âœ… Loaded model with {self.metadata['test_r2']:.1%} accuracy")
        
    def find_sparse_channels(self):
        """Find channels with sparse tracking data like 'I Like To Make Stuff'"""
        print("ðŸ” Finding sparse-data channels for testing...")
        
        # Load actual view snapshots data
        all_data = []
        batch_files = [f'data/ml_training_batch_{i}.json' for i in range(1, 15)]
        
        for batch_file in batch_files:
            try:
                with open(batch_file, 'r') as f:
                    batch_data = json.load(f)
                all_data.extend(batch_data)
            except FileNotFoundError:
                continue
        
        df = pd.DataFrame(all_data)
        
        # Convert types
        numeric_columns = ['subscriber_count', 'channel_video_count', 'view_count', 
                          'days_since_published', 'title_length', 'title_word_count',
                          'day_of_week', 'hour_of_day']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Find channels with sparse data (few snapshots per video)
        video_counts = df.groupby(['channel_name', 'video_id']).size()
        channel_stats = video_counts.groupby('channel_name').agg(['count', 'mean', 'max'])
        channel_stats.columns = ['video_count', 'avg_snapshots', 'max_snapshots']
        
        # Look for channels with few snapshots per video (sparse data)
        sparse_channels = channel_stats[
            (channel_stats['video_count'] >= 5) &      # At least 5 videos
            (channel_stats['avg_snapshots'] <= 3) &   # Few snapshots per video (sparse)
            (channel_stats['max_snapshots'] <= 5)     # Even best videos have few snapshots
        ]
        
        # Add subscriber info
        channel_details = []
        for channel in sparse_channels.index[:20]:  # Top 20 sparse channels
            channel_data = df[df['channel_name'] == channel]
            subscribers = channel_data['subscriber_count'].iloc[0]
            
            channel_details.append({
                'channel_name': channel,
                'video_count': sparse_channels.loc[channel, 'video_count'],
                'avg_snapshots_per_video': sparse_channels.loc[channel, 'avg_snapshots'],
                'subscriber_count': subscribers
            })
        
        channel_details_df = pd.DataFrame(channel_details).sort_values('subscriber_count', ascending=False)
        
        print(f"ðŸŽ¯ Found {len(sparse_channels)} sparse-data channels")
        print("\nTop sparse-data candidates (good for testing ML backfill):")
        for _, row in channel_details_df.head(10).iterrows():
            print(f"   {row['channel_name']}: {row['video_count']} videos, "
                  f"{row['avg_snapshots_per_video']:.1f} avg snapshots, "
                  f"{row['subscriber_count']:,} subs")
        
        # Try to find "I Like To Make Stuff" specifically
        target_channel = None
        if 'I Like To Make Stuff' in df['channel_name'].values:
            target_channel = 'I Like To Make Stuff'
            print(f"\nðŸŽ¯ Found target channel: {target_channel}")
        else:
            # Look for similar maker/DIY channels
            maker_keywords = ['make', 'diy', 'build', 'craft', 'woodwork', 'project']
            for keyword in maker_keywords:
                matching_channels = [ch for ch in channel_details_df['channel_name'].tolist() 
                                   if keyword.lower() in ch.lower()]
                if matching_channels:
                    target_channel = matching_channels[0]
                    print(f"ðŸŽ¯ Using similar channel: {target_channel}")
                    break
        
        if not target_channel:
            target_channel = channel_details_df.iloc[0]['channel_name']
            print(f"ðŸŽ¯ Using top sparse channel: {target_channel}")
        
        return target_channel, df[df['channel_name'] == target_channel]
    
    def predict_video_progression(self, video_info, days_to_predict):
        """Predict daily view progression for a video"""
        predictions = []
        
        for day in days_to_predict:
            # Create feature vector
            features = {
                'days_since_published': day,
                'log_subscriber_count': np.log1p(video_info['subscriber_count']),
                'channel_video_count': video_info['channel_video_count'],
                'log_channel_baseline': video_info['log_channel_baseline'],
                'title_length': video_info['title_length'],
                'title_word_count': video_info['title_word_count'],
                'day_of_week': video_info['day_of_week'],
                'hour_of_day': video_info['hour_of_day']
            }
            
            # Add encoded categorical features
            for cat_col in ['format_type', 'topic_domain', 'days_category', 'subscriber_tier']:
                if cat_col in self.preprocessors['encoders']:
                    try:
                        encoded_val = self.preprocessors['encoders'][cat_col].transform([str(video_info.get(cat_col, 'unknown'))])[0]
                        features[f'{cat_col}_encoded'] = encoded_val
                    except:
                        features[f'{cat_col}_encoded'] = 0
            
            # Create DataFrame and scale features
            feature_df = pd.DataFrame([features])
            
            # Scale numerical features
            numerical_cols = [col for col in self.metadata['feature_columns'] if not col.endswith('_encoded')]
            if len(numerical_cols) > 0:
                feature_df[numerical_cols] = self.preprocessors['scalers']['features'].transform(feature_df[numerical_cols])
            
            # Predict (model outputs log views)
            log_views_pred = self.model.predict(feature_df[self.metadata['feature_columns']])[0]
            views_pred = np.expm1(log_views_pred)  # Convert back from log
            
            predictions.append({
                'days_since_published': day,
                'predicted_views': max(views_pred, 100)  # Ensure minimum reasonable views
            })
        
        return pd.DataFrame(predictions)
    
    def generate_ml_confidence_bands(self, channel_data):
        """Generate ML-enhanced confidence bands for channel videos"""
        print(f"ðŸ“Š Generating ML-enhanced confidence bands...")
        
        # Get channel baseline from available data
        channel_baseline = channel_data['view_count'].median()
        channel_name = channel_data['channel_name'].iloc[0]
        
        print(f"ðŸ“Š Channel: {channel_name}")
        print(f"ðŸ“Š Available data: {len(channel_data)} snapshots from {channel_data['video_id'].nunique()} videos")
        print(f"ðŸ“Š Channel baseline: {channel_baseline:,.0f} views")
        
        # Get unique videos and their characteristics
        videos = []
        for video_id in channel_data['video_id'].unique()[:5]:  # Use up to 5 videos
            video_snapshots = channel_data[channel_data['video_id'] == video_id]
            if len(video_snapshots) > 0:
                video_info = video_snapshots.iloc[0].to_dict()
                video_info['log_channel_baseline'] = np.log1p(channel_baseline)
                video_info['days_category'] = 'month1'  # Most videos fall in this range
                
                # Determine subscriber tier
                subs = video_info['subscriber_count']
                if subs < 10000:
                    video_info['subscriber_tier'] = 'small'
                elif subs < 100000:
                    video_info['subscriber_tier'] = 'medium'
                elif subs < 1000000:
                    video_info['subscriber_tier'] = 'large'
                else:
                    video_info['subscriber_tier'] = 'mega'
                
                videos.append(video_info)
        
        # Generate progression curves for each video
        days_range = list(range(1, 91))  # 90 days
        all_progressions = []
        
        print(f"ðŸ¤– Generating ML progressions for {len(videos)} videos...")
        
        for i, video_info in enumerate(videos):
            # Add some variation to create realistic spread
            video_variant = video_info.copy()
            
            # Vary title characteristics slightly
            base_title_length = video_info['title_length']
            video_variant['title_length'] = max(10, base_title_length * (0.8 + i * 0.1))
            video_variant['title_word_count'] = max(2, int(video_variant['title_length'] / 6))
            
            # Vary publish timing
            video_variant['hour_of_day'] = (video_info['hour_of_day'] + i * 2) % 24
            video_variant['day_of_week'] = (video_info['day_of_week'] + i) % 7
            
            progression = self.predict_video_progression(video_variant, days_range)
            progression['video_num'] = i
            progression['video_id'] = f"video_{i+1}"
            all_progressions.append(progression)
        
        # Combine all progressions
        all_progressions_df = pd.concat(all_progressions, ignore_index=True)
        
        # Calculate percentiles for confidence bands
        confidence_bands = all_progressions_df.groupby('days_since_published')['predicted_views'].agg([
            lambda x: np.percentile(x, 10),
            lambda x: np.percentile(x, 50), 
            lambda x: np.percentile(x, 90)
        ]).reset_index()
        
        confidence_bands.columns = ['days_since_published', 'p10', 'p50', 'p90']
        
        return confidence_bands, all_progressions_df, channel_data
    
    def create_ml_vs_global_comparison(self, channel_data, confidence_bands):
        """Compare ML-enhanced bands vs global curve scaling approach"""
        print("ðŸ“Š Creating ML vs Global Curve comparison...")
        
        # Simulate your current global curve scaling approach
        # Using median values from all data as "global baseline"
        global_baseline = 50000  # Typical global baseline
        channel_baseline = channel_data['view_count'].median()
        scale_factor = channel_baseline / global_baseline
        
        # Global performance envelope (simplified)
        days = confidence_bands['days_since_published']
        global_p10 = global_baseline * 0.3 * scale_factor  # 30% of baseline
        global_p50 = global_baseline * scale_factor         # Baseline
        global_p90 = global_baseline * 2.5 * scale_factor  # 250% of baseline
        
        # Create comparison visualization
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        
        # Left: ML-Enhanced Confidence Bands
        ax1.fill_between(confidence_bands['days_since_published'], 
                        confidence_bands['p10'], confidence_bands['p90'],
                        alpha=0.3, color='blue', label='ML-Enhanced Confidence Band')
        ax1.plot(confidence_bands['days_since_published'], confidence_bands['p50'],
                'b-', linewidth=3, label='ML Expected Performance')
        
        # Plot actual data points
        for video_id in channel_data['video_id'].unique():
            video_snapshots = channel_data[channel_data['video_id'] == video_id]
            ax1.scatter(video_snapshots['days_since_published'], video_snapshots['view_count'],
                       alpha=0.7, s=60, label=f'Actual Data' if video_id == channel_data['video_id'].iloc[0] else "")
        
        ax1.set_xlabel('Days Since Published')
        ax1.set_ylabel('View Count')
        ax1.set_title('ML-Enhanced Confidence Bands\n(Channel-Specific Patterns)')
        ax1.set_yscale('log')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # Right: Current Global Curve Scaling
        ax2.fill_between(days, [global_p10] * len(days), [global_p90] * len(days),
                        alpha=0.3, color='red', label='Global Curve Confidence Band')
        ax2.axhline(y=global_p50, color='red', linewidth=3, label='Global Expected Performance')
        
        # Plot same actual data points
        for video_id in channel_data['video_id'].unique():
            video_snapshots = channel_data[channel_data['video_id'] == video_id]
            ax2.scatter(video_snapshots['days_since_published'], video_snapshots['view_count'],
                       alpha=0.7, s=60, label=f'Actual Data' if video_id == channel_data['video_id'].iloc[0] else "")
        
        ax2.set_xlabel('Days Since Published')
        ax2.set_ylabel('View Count')
        ax2.set_title('Current Global Curve Scaling\n(One-Size-Fits-All)')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        # Add channel info
        channel_name = channel_data['channel_name'].iloc[0]
        subscriber_count = channel_data['subscriber_count'].iloc[0]
        fig.suptitle(f'Performance Confidence Bands Comparison: {channel_name}\n'
                    f'({subscriber_count:,} subscribers, {channel_data["video_id"].nunique()} videos tracked)', 
                    fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        comparison_path = f'data/ml_vs_global_comparison_{self.model_timestamp}.png'
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        print(f"ðŸ’¾ Comparison saved: {comparison_path}")
        
        # Calculate band widths for comparison
        ml_band_width = np.mean(confidence_bands['p90'] - confidence_bands['p10'])
        global_band_width = global_p90 - global_p10
        
        print(f"\nðŸ“Š CONFIDENCE BAND ANALYSIS:")
        print(f"   ML Band Width: {ml_band_width:,.0f} views (adaptive)")
        print(f"   Global Band Width: {global_band_width:,.0f} views (fixed)")
        print(f"   ML vs Global: {(ml_band_width/global_band_width-1)*100:+.1f}% difference")
        
        return comparison_path
    
    def analyze_video_performance_assessment(self, channel_data, confidence_bands):
        """Analyze how well each approach identifies over/under performing videos"""
        print("ðŸŽ¯ Analyzing video performance assessment accuracy...")
        
        results = []
        
        for video_id in channel_data['video_id'].unique():
            video_snapshots = channel_data[channel_data['video_id'] == video_id]
            
            for _, snapshot in video_snapshots.iterrows():
                day = snapshot['days_since_published']
                actual_views = snapshot['view_count']
                
                # Find ML prediction for this day
                ml_day_data = confidence_bands[confidence_bands['days_since_published'].between(day-5, day+5)]
                if len(ml_day_data) > 0:
                    ml_p10 = ml_day_data['p10'].mean()
                    ml_p50 = ml_day_data['p50'].mean()
                    ml_p90 = ml_day_data['p90'].mean()
                    
                    # ML assessment
                    if actual_views > ml_p90:
                        ml_assessment = "Overperforming"
                    elif actual_views < ml_p10:
                        ml_assessment = "Underperforming"
                    else:
                        ml_assessment = "Meeting expectations"
                    
                    # Global assessment (simplified)
                    channel_baseline = channel_data['view_count'].median()
                    if actual_views > channel_baseline * 2:
                        global_assessment = "Overperforming"
                    elif actual_views < channel_baseline * 0.5:
                        global_assessment = "Underperforming"
                    else:
                        global_assessment = "Meeting expectations"
                    
                    results.append({
                        'video_id': video_id,
                        'days_since_published': day,
                        'actual_views': actual_views,
                        'ml_assessment': ml_assessment,
                        'global_assessment': global_assessment,
                        'ml_p50': ml_p50,
                        'channel_baseline': channel_baseline
                    })
        
        results_df = pd.DataFrame(results)
        
        if len(results_df) > 0:
            print(f"ðŸ“Š Performance Assessments:")
            ml_assessments = results_df['ml_assessment'].value_counts()
            global_assessments = results_df['global_assessment'].value_counts()
            
            print(f"   ML Approach:")
            for assessment, count in ml_assessments.items():
                print(f"     {assessment}: {count} videos")
            
            print(f"   Global Approach:")
            for assessment, count in global_assessments.items():
                print(f"     {assessment}: {count} videos")
            
            # Agreement analysis
            agreement = (results_df['ml_assessment'] == results_df['global_assessment']).mean()
            print(f"   Assessment Agreement: {agreement:.1%}")
        
        return results_df

def main():
    print("ðŸš€ Applying Historical Backfill to Real Sparse-Data Channel")
    print("=" * 70)
    
    tester = RealChannelBackfillTester()
    
    # Load models
    tester.load_models()
    
    # Find sparse-data channels
    target_channel, channel_data = tester.find_sparse_channels()
    
    # Generate ML-enhanced confidence bands
    confidence_bands, progressions, channel_data = tester.generate_ml_confidence_bands(channel_data)
    
    # Create comparison visualization
    comparison_path = tester.create_ml_vs_global_comparison(channel_data, confidence_bands)
    
    # Analyze performance assessments
    assessment_results = tester.analyze_video_performance_assessment(channel_data, confidence_bands)
    
    # Summary
    print("\n" + "=" * 70)
    print("ðŸŽ‰ ML-Enhanced Confidence Bands Generated!")
    print(f"âœ… Target Channel: {target_channel}")
    print(f"ðŸ“Š Generated confidence bands using ML backfill of {channel_data['video_id'].nunique()} videos")
    print(f"ðŸ“ˆ ML bands adapt to channel-specific patterns vs fixed global scaling")
    print(f"ðŸ’¾ Comparison visualization: {comparison_path}")
    print("ðŸŽ¯ Ready to apply to your performance graph!")
    print("=" * 70)

if __name__ == "__main__":
    main()